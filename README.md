# ç¬¬äºŒç« 

### tf.where()

æ¡ä»¶è¯­å¥çœŸè¿”å›A,æ¡ä»¶è¯­å¥å‡è¿”å›B

tf.where(æ¡ä»¶è¯­å¥, çœŸè¿”å›A, å‡è¿”å›B)

```python
import tensorflow as tf

a = tf.constant([1, 2, 3, 1, 1])
b = tf.constant([0, 1, 3, 4, 5])
c = tf.where(tf.greater(a, b), a, b)  # è‹¥ a > b, è¿”å› a å¯¹åº”ä½ç½®çš„å…ƒç´ ,å¦åˆ™è¿”å› b å¯¹åº”çš„å…ƒç´ 
print('c:', c)
```

```python
è¿è¡Œç»“æœ:
c: tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)
```



### np.random.RandomState.rand()
è¿”å›ä¸€ä¸ª [0, 1) ä¹‹é—´çš„éšæœºæ•°
np.random.RandomState.rand(ç»´åº¦)  # ç»´åº¦ä¸ºç©ºï¼Œè¿”å›æ ‡é‡

```python
import numpy as np
rdm = np.random.RandomState(seed=1)  # seed å®šä¹‰ç”Ÿæˆéšæœºæ•°
a = rdm.rand()  # è¿”å›ä¸€ä¸ªæ ‡é‡
b = rdm.rand(2, 3) # è¿”å›ç»´åº¦ä¸º2è¡Œ3åˆ—éšæœºæ•°çŸ©é˜µ
print('a:', a)
print('b:', b)
```
```
è¿è¡Œç»“æœ:
a: 0.417022004702574
b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]
	[1.46755891e-01 9.23385948e-02 1.86260211e-01]]
```

### np.vstack()

å°†ä¸¤ä¸ªæ•°ç»„æŒ‰å‚ç›´æ–¹å‘å åŠ 

np.vstack(æ•°ç»„1ï¼Œ æ•°ç»„2)

```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c = np.vstack((a, b))
print('c:', c)
```

```
è¿è¡Œç»“æœ:
c: [[1 2 3]
 	[4 5 6]]
```

### np.mgrid[]   .ravle()    np.c_[]

np.mgrid[]            [èµ·å§‹å€¼, ç»“æŸå€¼)

np.mgrid[èµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿, èµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿, ...]

.ravle()   å°†xå˜ä¸ºä¸€ç»´æ•°ç»„ï¼Œâ€æŠŠ .å‰å˜é‡æ‹‰ç›´â€œ

np.c_[]  ä½¿è¿”å›çš„é—´éš”æ•°å€¼ç‚¹é…å¯¹

np.c_[æ•°ç»„1, æ•°ç»„2, ...]

```python
import numpy as np
x, y = np.mgrid[1: 3: 1, 2: 4: 0.5]
grid = np.c_[x.ravel(), y.ravel()]
print('x:', x)
print('y:', y)
print('grid:\n', grid)
```

```
è¿è¡Œç»“æœ:
x: [[1. 1. 1. 1.]
	[2. 2. 2. 2.]]
y: [[2.  2.5 3.  3.5]
	[2.  2.5 3.  3.5]]
grid:
 [[1.  2. ]
  [1.  2.5]
  [1.  3. ]
  [1.  3.5]
  [2.  2. ]
  [2.  2.5]
  [2.  3. ]
  [2.  3.5]]
```

## ç¥ç»ç½‘ç»œçš„å¤æ‚åº¦

âœ” NNå¤æ‚åº¦ï¼šå¤šç”¨NNå±‚æ•°å’ŒNNå‚æ•°çš„ä¸ªæ•°è¡¨ç¤º

![image-20210315213822119](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210315213822119.png)

**ç©ºé—´å¤æ‚åº¦ï¼š**

âœ” å±‚æ•° = éšè—å±‚çš„å±‚æ•° + 1ä¸ªè¾“å‡ºå±‚

<u>ä¸Šå›¾ä¸º2å±‚NN</u>

âœ” æ€»å‚æ•° = æ€»w + æ€»b

<u>ä¸Šå›¾ä¸º3 * 4 + 4 + 4 * 2 + 2 = 26</u>

**æ—¶é—´å¤æ‚åº¦ï¼š**

âœ” ä¹˜åŠ è¿ç®—æ¬¡æ•°

<u>ä¸Šå›¾ 3 * 4 + 4 * 2 = 20</u>



### å­¦ä¹ ç‡

å­¦ä¹ ç‡lr=0.001è¿‡æ…¢, lr=0.999ä¸æ”¶æ•›

**æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡**

å¯ä»¥å…ˆç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼Œå¿«é€Ÿå¾—åˆ°è¾ƒä¼˜è§£ï¼Œç„¶åé€æ­¥å‡å°å­¦ä¹ ç‡ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒåæœŸç¨³å®šã€‚

æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ = åˆå§‹å­¦ä¹ ç‡ * å­¦ä¹ è¡°å‡ç‡ ^ (å½“å‰è½®æ•° / å¤šå°‘è½®è¡°å‡ä¸€æ¬¡)

```python
epoch = 40
LR_BASE = 0.2
LRDECAY = 0.99
LR_STEP = 1

for epoch in range(epoch):
    lr = LR_BASE * LR_DECAY  ** (epoch / LR_STEP)
    with tf.GradientTape() as tape:
        loss = tf.square(w + 1)
    grads = tape.gradient(loss, w)
    
    w.assign_sub(lr * grads)
    print("After %s epoch, w is %f, loss is %f, lr is %f" % (epoch, w.numpy(), loss, lr))
```



### æ¿€æ´»å‡½æ•°

âœ” Sigmoidå‡½æ•°

$$
f(x)=\frac{1}{1+e^{-x}}
$$
tf.nn.sig,oid(x)  # å°†è¾“å…¥å€¼å˜æ¢åˆ° (0, 1) ä¹‹é—´è¾“å‡º

![image-20210316090549966](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316090549966.png)



âœ” Tanhå‡½æ•°
$$
f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}
$$
tf.math.tanh(x)

![image-20210316091148114](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316091148114.png)

ç‰¹ç‚¹

1. è¾“å‡ºæ˜¯0å‡å€¼
2. æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±
3. å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿



âœ” Reluå‡½æ•°
$$
f(x)=max(x, 0)=
$$
tf.nn.relu(x)

![image-20210316091544443](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316091544443.png)



âœ” Leaky Relu

tf.nn.leaky_relu(x)

![image-20210316091802144](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316091802144.png)



**å¯¹äºåˆå­¦è€…çš„å»ºè®®**

![image-20210316091926516](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316091926516.png)





âœ” æŸå¤±å‡½æ•° (loss): é¢„æµ‹å€¼ (y) ä¸å·²çŸ¥ç­”æ¡ˆ (y_) çš„å·®è·

NNä¼˜åŒ–ç›®æ ‡ï¼š lossæœ€å° 

* mse(Mean Squared Error)
* è‡ªå®šä¹‰
* ce(Cross Entropy)  # äº¤å‰ç†µ

âœ” å‡æ–¹è¯¯å·® mse
$$
MSE({y_\_},y)=\frac{\sum_{i=1}^n (y - y_\_)^2}{n}
$$
loss_mse = tf.reduce_mean(tf.square(y_ -  y))

![image-20210316092407331](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316092407331.png)

ğŸ’¬ Example

é¢„æµ‹é…¸å¥¶æ—¥é”€é‡y, x1, x2 æ˜¯å½±å“æ—¥é”€é‡çš„å› ç´ ã€‚

å»ºæ¨¡å‰ï¼Œåº”é¢„å…ˆé‡‡é›†çš„æ•°æ®æœ‰ï¼šæ¯æ—¥x1ã€x2å’Œé”€é‡y_ ï¼ˆå³å·²çŸ¥ç­”æ¡ˆï¼Œæœ€ä½³æƒ…å†µï¼šäº§é‡=é”€é‡ï¼‰

æ‹Ÿé€ æ•°æ®é›†X, Y_: y_ = x1 + x2   å™ªå£°ï¼š-0.05 ~ +0.05   æ‹Ÿåˆå¯ä»¥é¢„æµ‹é”€é‡çš„å‡½æ•°

```python
import tensorflow as tf
import numpy as np

SEED = 23455

rdm = np.random.RandomState(seed=SEED)

x = rdm.rand(32, 2)

y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for x1, x2 in x]  # ç”Ÿæˆå™ªå£°[0, 1) / 10 = [0, 0.1)

x = tf.cast(x, dtype=tf.float32)

w1 = tf.Variable(tf.random.normal([2, 1], stddev=1, seed=1))

epoch = 15000
lr = 0.002

for epoch in range(epoch):
    with tf.GradientTape() as tape:
        y = tf.matmul(x, w1)
        loss_mse = tf.reduce_mean(tf.square(y_ - y))
    
    grads = tape.gradient(loss_mse, w1)
    w1.assign_sub(lr * grads)

    if epoch % 500 == 0:
        print("After %d training steps, w1 is " % (epoch))
        print(w1.numpy())
print('Final w1 is:\n', w1.numpy())
```

```
è¿è¡Œç»“æœï¼š
After 14500 training steps, w1 is 
[[1.0002553 ]
 [0.99838644]]
Final w1 is:
 [[1.0009792]
  [0.9977485]]
```



âœ” è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼š

å¦‚é¢„æµ‹å•†å“é”€é‡ï¼Œé¢„æµ‹å¤šäº†ï¼ŒæŸå¤±æˆæœ¬ï¼Œé¢„æµ‹å°‘äº†ï¼ŒæŸå¤±åˆ©æ¶¦ã€‚

è‹¥åˆ©æ¶¦ â‰  æˆæœ¬ï¼Œåˆ™ mse äº§ç”Ÿçš„ loss æ— æ³•åˆ©ç›Šæœ€å¤§åŒ–

è‡ªå®šä¹‰æŸå¤±å‡½æ•°  loss(y_, y) = 
$$
loss(y_\_,y)=\sum_{n}{f(y_\_,y)}
$$
![image-20210316094809367](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210316094809367.png)

 

âœ” äº¤å‰ç†µæŸå¤±å‡½æ•° CE (Cross Entropy): è¡¨å¾ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»
$$
H(y_\_,y)=-\sum{y_\_*\ln y}
$$
eg. äºŒåˆ†ç±»å·²çŸ¥ç­”æ¡ˆ y_ = (1, 0)    é¢„æµ‹ y1=(0.6, 0.5)  y2 = (0.8, 0.2), å“ªä¸ªæ›´æ¥è¿‘æ ‡å‡†ç­”æ¡ˆï¼Ÿ
$$
H_1((1, 0), (0.6, 0.4)) = -(1 * \ln0.6 + 0 * \ln0.4)\approx -(-0.511 +0)=0.511\\
H_2((1, 0), (0.8, 0.2)) = -(1 * \ln0.8 + 0 * \ln0.2)\approx -(-0.223 +0)=0.223
$$
å› ä¸º H1 > H2ï¼Œæ‰€ä»¥ y2 é¢„æµ‹æ›´å‡†

tf.losses.categorical_crossentropy(y_, y)

```python
import tensorflow as tf

loss_ce1 = tf.losses.categorical_crossentropy([1, 0], [0.6, 0.4])
loss_ce2 = tf.losses.categorical_crossentropy([1, 0], [0.8, 0.2])
print('loss_ce1:', loss_ce1)
print('loss_ce2:', loss_ce2)
```

```
è¿è¡Œç»“æœï¼š

loss_ce1: tf.Tensor(0.5108256, shape=(), dtype=float32)
loss_ce2: tf.Tensor(0.22314353, shape=(), dtype=float32)
```

![image-20210317085531909](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210317085531909.png)

### softmax ä¸äº¤å‰ç†µç»“åˆ

âœ” è¾“å‡ºå…ˆè¿‡softmaxå‡½æ•°ï¼Œå†è®¡ç®—yä¸y_çš„äº¤å‰ç†µæŸå¤±å‡½æ•°

tf.nn.softmax_cross_entropy_with_logits(y_, y)

```python
import tensorflow as tf
import numpy as np

y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1],[1, 0, 0], [0, 1, 0]])
y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])
y_pro = tf.nn.softmax(y)
loss_ce1 = tf.losses.categorical_crossentropy(y_, y_pro)
loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)
print('åˆ†å¸ƒè®¡ç®—çš„ç»“æœï¼š\n', loss_ce1)
print('ç»“åˆè®¡ç®—çš„ç»“æœï¼š\n', loss_ce2)
```

```
è¿è¡Œç»“æœï¼š
åˆ†å¸ƒè®¡ç®—çš„ç»“æœï¼š
 tf.Tensor(
[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
 5.49852354e-02], shape=(5,), dtype=float64)
ç»“åˆè®¡ç®—çš„ç»“æœï¼š
 tf.Tensor(
[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
 5.49852354e-02], shape=(5,), dtype=float64)
```



## 2.5 ç¼“è§£è¿‡æ‹Ÿåˆ

![image-20210317090543891](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210317090543891.png)

âœ”  æ¬ æ‹Ÿåˆçš„è§£å†³æ–¹æ³•:

* å¢åŠ è¾“å…¥ç‰¹å¾é¡¹
* å¢åŠ ç½‘ç»œå‚æ•°
* å‡å°‘æ­£åˆ™åŒ–å‚æ•°

âœ” è¿‡æ‹Ÿåˆçš„è§£å†³æ–¹æ³•:

* æ•°æ®æ¸…æ´—
* å¢å¤§è®­ç»ƒé›†
* é‡‡ç”¨æ­£åˆ™åŒ–
* å¢å¤§æ­£åˆ™åŒ–å‚æ•°

âœ” æ­£åˆ™åŒ–ç¼“è§£è¿‡æ‹Ÿåˆ

![image-20210317092518987](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210317092518987.png)

![image-20210317092729741](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20210317092729741.png)



ä»£ç 





## 2.6 ä¼˜åŒ–å™¨

### ç¥ç»ç½‘ç»œå‚æ•°ä¼˜åŒ–å™¨

å¾…ä¼˜åŒ–å‚æ•°wï¼ŒæŸå¤±å‡½æ•°lossï¼Œå­¦ä¹ ç‡lrï¼Œæ¯æ¬¡è¿­ä»£ä¸€ä¸ªbatchï¼Œt è¡¨ç¤ºå½“å‰batchè¿­ä»£çš„æ€»æ¬¡æ•°ï¼š

1. è®¡ç®— t æ—¶åˆ»æŸå¤±å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦ $ g_t=\nabla loss = \frac{\partial loss}{\partial (w_t)} $
2. è®¡ç®— t æ—¶åˆ»ä¸€é˜¶åŠ¨é‡ $m_t$ å’ŒäºŒé˜¶åŠ¨é‡ $V_t$
3. è®¡ç®— t æ—¶åˆ»ä¸‹é™æ¢¯åº¦ï¼š$\eta_t = lr * m_t / \sqrt{V_t}$
4. è®¡ç®— t + 1 æ—¶åˆ»å‚æ•°ï¼š$w_{t+1} = w_t - \eta _t = w_t - lr * m_t / \sqrt{V_t}$

**ä¸€é˜¶åŠ¨é‡**ï¼šä¸æ¢¯åº¦ç›¸å…³çš„å‡½æ•°

**äºŒé˜¶åŠ¨é‡**ï¼šä¸æ¢¯åº¦å¹³æ–¹ç›¸å…³çš„å‡½æ•°

### ä¼˜åŒ–å™¨

âœ” SGDï¼ˆæ—  momentumï¼‰ï¼Œå¸¸ç”¨çš„æ¢¯åº¦ä¸‹é™æ³•ã€‚

$m_t = g_t \ \ \ \ \ \ \ \  V_T = 1$

$\eta _t = lr * m_t / \sqrt{V_t} = lr * g_t$

$w_{t+1} = w_t - \eta _t = w_t - lr * m_t / \sqrt{V_t} = w_t -lr * g_t$

$w_{t+1} = w_t - lr * \frac {\partial loss} {\partial w_t}$

```python
# SGD
w1.assign_sub(lr * grads[0])  # å‚æ•°w1è‡ªæ›´æ–°
b1.assign_sub(lr * grads[1])  # å‚æ•°b1è‡ªæ›´æ–°
```

```python
# åˆ©ç”¨é¸¢å°¾èŠ±æ•°æ®é›†ï¼Œå®ç°å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ï¼Œå¯è§†åŒ–lossæ›²çº¿

# å¯¼å…¥æ‰€éœ€æ¨¡å—
import tensorflow as tf
from sklearn import datasets
from matplotlib import pyplot as plt
import numpy as np
import time  ##1##

# å¯¼å…¥æ•°æ®ï¼Œåˆ†åˆ«ä¸ºè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target

# éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå› ä¸ºåŸå§‹æ•°æ®æ˜¯é¡ºåºçš„ï¼Œé¡ºåºä¸æ‰“ä¹±ä¼šå½±å“å‡†ç¡®ç‡ï¼‰
# seed: éšæœºæ•°ç§å­ï¼Œæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå½“è®¾ç½®ä¹‹åï¼Œæ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°éƒ½ä¸€æ ·ï¼ˆä¸ºæ–¹ä¾¿æ•™å­¦ï¼Œä»¥ä¿æ¯ä½åŒå­¦ç»“æœä¸€è‡´ï¼‰
np.random.seed(116)  # ä½¿ç”¨ç›¸åŒçš„seedï¼Œä¿è¯è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ä¸€ä¸€å¯¹åº”
np.random.shuffle(x_data)
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)

# å°†æ‰“ä¹±åçš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸ºå‰120è¡Œï¼Œæµ‹è¯•é›†ä¸ºå30è¡Œ
x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]

# è½¬æ¢xçš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™åé¢çŸ©é˜µç›¸ä¹˜æ—¶ä¼šå› æ•°æ®ç±»å‹ä¸ä¸€è‡´æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)

# from_tensor_sliceså‡½æ•°ä½¿è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾å€¼ä¸€ä¸€å¯¹åº”ã€‚ï¼ˆæŠŠæ•°æ®é›†åˆ†æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡batchç»„æ•°æ®ï¼‰
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œ4ä¸ªè¾“å…¥ç‰¹å¾æ•…ï¼Œè¾“å…¥å±‚ä¸º4ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼›å› ä¸º3åˆ†ç±»ï¼Œæ•…è¾“å‡ºå±‚ä¸º3ä¸ªç¥ç»å…ƒ
# ç”¨tf.Variable()æ ‡è®°å‚æ•°å¯è®­ç»ƒ
# ä½¿ç”¨seedä½¿æ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°ç›¸åŒï¼ˆæ–¹ä¾¿æ•™å­¦ï¼Œä½¿å¤§å®¶ç»“æœéƒ½ä¸€è‡´ï¼Œåœ¨ç°å®ä½¿ç”¨æ—¶ä¸å†™seedï¼‰
w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))

lr = 0.1  # å­¦ä¹ ç‡ä¸º0.1
train_loss_results = []  # å°†æ¯è½®çš„lossè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»lossæ›²çº¿æä¾›æ•°æ®
test_acc = []  # å°†æ¯è½®çš„accè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»accæ›²çº¿æä¾›æ•°æ®
epoch = 500  # å¾ªç¯500è½®
loss_all = 0  # æ¯è½®åˆ†4ä¸ªstepï¼Œloss_allè®°å½•å››ä¸ªstepç”Ÿæˆçš„4ä¸ªlossçš„å’Œ

# è®­ç»ƒéƒ¨åˆ†
now_time = time.time()  ##2##
for epoch in range(epoch):  # æ•°æ®é›†çº§åˆ«çš„å¾ªç¯ï¼Œæ¯ä¸ªepochå¾ªç¯ä¸€æ¬¡æ•°æ®é›†
    for step, (x_train, y_train) in enumerate(train_db):  # batchçº§åˆ«çš„å¾ªç¯ ï¼Œæ¯ä¸ªstepå¾ªç¯ä¸€ä¸ªbatch
        with tf.GradientTape() as tape:  # withç»“æ„è®°å½•æ¢¯åº¦ä¿¡æ¯
            y = tf.matmul(x_train, w1) + b1  # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)  # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)  # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            loss = tf.reduce_mean(tf.square(y_ - y))  # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()  # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        # è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = tape.gradient(loss, [w1, b1])

        # å®ç°æ¢¯åº¦æ›´æ–° w1 = w1 - lr * w1_grad    b = b - lr * b_grad
        w1.assign_sub(lr * grads[0])  # å‚æ•°w1è‡ªæ›´æ–°
        b1.assign_sub(lr * grads[1])  # å‚æ•°bè‡ªæ›´æ–°

    # æ¯ä¸ªepochï¼Œæ‰“å°lossä¿¡æ¯
    print("Epoch {}, loss: {}".format(epoch, loss_all / 4))
    train_loss_results.append(loss_all / 4)  # å°†4ä¸ªstepçš„lossæ±‚å¹³å‡è®°å½•åœ¨æ­¤å˜é‡ä¸­
    loss_all = 0  # loss_allå½’é›¶ï¼Œä¸ºè®°å½•ä¸‹ä¸€ä¸ªepochçš„lossåšå‡†å¤‡

    # æµ‹è¯•éƒ¨åˆ†
    # total_correctä¸ºé¢„æµ‹å¯¹çš„æ ·æœ¬ä¸ªæ•°, total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œå°†è¿™ä¸¤ä¸ªå˜é‡éƒ½åˆå§‹åŒ–ä¸º0
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        y = tf.matmul(x_test, w1) + b1
        y = tf.nn.softmax(y)
        pred = tf.argmax(y, axis=1)  # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„åˆ†ç±»
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        # è‹¥åˆ†ç±»æ­£ç¡®ï¼Œåˆ™correct=1ï¼Œå¦åˆ™ä¸º0ï¼Œå°†boolå‹çš„ç»“æœè½¬æ¢ä¸ºintå‹
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        # å°†æ¯ä¸ªbatchçš„correctæ•°åŠ èµ·æ¥
        correct = tf.reduce_sum(correct)
        # å°†æ‰€æœ‰batchä¸­çš„correctæ•°åŠ èµ·æ¥
        total_correct += int(correct)
        # total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œä¹Ÿå°±æ˜¯x_testçš„è¡Œæ•°ï¼Œshape[0]è¿”å›å˜é‡çš„è¡Œæ•°
        total_number += x_test.shape[0]
    # æ€»çš„å‡†ç¡®ç‡ç­‰äºtotal_correct/total_number
    acc = total_correct / total_number
    test_acc.append(acc)
    print("Test_acc:", acc)
    print("--------------------------")
total_time = time.time() - now_time  ##3##
print("total_time", total_time)  ##4##

# ç»˜åˆ¶ loss æ›²çº¿
plt.title('Loss Function Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Loss')  # yè½´å˜é‡åç§°
plt.plot(train_loss_results, label="$Loss$")  # é€ç‚¹ç”»å‡ºtrian_loss_resultså€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Loss
plt.legend()  # ç”»å‡ºæ›²çº¿å›¾æ ‡
plt.show()  # ç”»å‡ºå›¾åƒ

# ç»˜åˆ¶ Accuracy æ›²çº¿
plt.title('Acc Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Acc')  # yè½´å˜é‡åç§°
plt.plot(test_acc, label="$Accuracy$")  # é€ç‚¹ç”»å‡ºtest_accå€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Accuracy
plt.legend()
plt.show()

# æœ¬æ–‡ä»¶è¾ƒ class1\p45_iris.py ä»…æ·»åŠ å››å¤„æ—¶é—´è®°å½•  ç”¨ ##n## æ ‡è¯†
# è¯·å°†lossæ›²çº¿ã€ACCæ›²çº¿ã€total_timeè®°å½•åˆ° class2\ä¼˜åŒ–å™¨å¯¹æ¯”.docx  å¯¹æ¯”å„ä¼˜åŒ–å™¨æ”¶æ•›æƒ…å†µ
```

```
è¿è¡Œç»“æœï¼š
--------------------------
Epoch 498, loss: 0.03232627175748348
Test_acc: 1.0
--------------------------
Epoch 499, loss: 0.032300276681780815
Test_acc: 1.0
--------------------------
total_time 12.701475381851196
```



âœ” SGDMï¼ˆå«momentumçš„SGDï¼‰ï¼Œåœ¨SGDåŸºç¡€ä¸Šå¢åŠ ä¸€é˜¶åŠ¨é‡ã€‚

$m_t = \beta * m_{t-1} + (1-\beta)*g_t \ \ \ \ \ \ \ \  V_T = 1$

$\eta _t = lr * m_t / \sqrt{V_t} = lr * m_t = lr*(\beta*m_{t-1}+(1-\beta)*g_t)$

$w_{t+1} = w_t - \eta _t = w_t - lr * (\beta*m_{t-1}+(1-\beta)*g_t$



**Codes:**

$m_t = \beta * m_{t-1} + (1-\beta)*g_t \ \ \ \ \ \ \ \  V_T = 1$

```python
# ä¸SGDä¸åŒä¹‹å¤„
m_w, m_b = 0, 0
beta = 0.9
# sgd-momentum
m_w = beta * m_w + (1 - beta) * grads[0]
m_b = beta * m_b + (1 - beta) * grads[1]
w1.assign_sub(lr * m_w)
b1.assign_sub(lr * m_b)
```

```
è¿è¡Œç»“æœï¼š
--------------------------
Epoch 498, loss: 0.03065542411059141
Test_acc: 1.0
--------------------------
Epoch 499, loss: 0.030630568508058786
Test_acc: 1.0
--------------------------
total_time 10.623216390609741
```



âœ” Adagradï¼Œåœ¨SGDåŸºç¡€ä¸Šå¢åŠ äºŒé˜¶åŠ¨é‡

$$m_t = g_t \   \ \ \ \ \ \ V_t =\sum_{\tau=1}^t g_\tau ^2$$

$\eta_t = lr * m_t / (\sqrt{V_t})=lr*g_t / (\sqrt{\sum_{\tau=1}^t g_\tau ^2})$

$$w_{t+1} = w_t - \eta_t = w_t - lr * g_t/ (\sqrt{\sum_{\tau=1}^t g_\tau ^2})$$

```python
# ä¸SGDä¸åŒä¹‹å¤„ï¼š
v_w, v_b = 0, 0
# adagrad
v_w += tf.square(grads[0])
v_b += tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```

```
è¿è¡Œç»“æœï¼š
--------------------------
Epoch 498, loss: 0.03176002576947212
Test_acc: 1.0
--------------------------
Epoch 499, loss: 0.03173917159438133
Test_acc: 1.0
--------------------------
total_time 7.388378620147705
```



âœ” RMSPropï¼ŒSGDåŸºç¡€ä¸Šå¢åŠ äºŒé˜¶åŠ¨é‡

$$m_t = g_t \   \ \ \ \ \ \ V_t =\beta * V_{t-1}+(1-\beta)*g_t ^2$$

$\eta_t = lr * m_t / (\sqrt{V_t})=lr*g_t / (\sqrt{\beta * V_{t-1}+(1-\beta)*g_t ^2})$

$$w_{t+1} = w_t - \eta_t = w_t - lr * g_t/ (\sqrt{\beta * V_{t-1}+(1-\beta)*g_t ^2})$$

```python
# ä¸SGDä¸åŒä¹‹å¤„
v_w, v_b = 0, 0
beta = 0.9
# rmsprop
v_w = beta * v_w + (1 - beta) * tf.square(grads[0])
v_b = beta * v_b + (1 - beta) * tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```

```
è¿è¡Œç»“æœï¼š
--------------------------
Epoch 498, loss: nan
Test_acc: 0.36666666666666664
--------------------------
Epoch 499, loss: nan
Test_acc: 0.36666666666666664
--------------------------
total_time 12.019102573394775
```



âœ” Adamï¼ŒåŒæ—¶ç»“åˆSGDMä¸€é˜¶åŠ¨é‡å’ŒRMSPropäºŒé˜¶åŠ¨é‡

$m_t = \beta_1*m_{t-1}+(1-\beta_1)*g_t$

ä¿®æ­£ä¸€é˜¶åŠ¨é‡çš„åå·®ï¼š$\hat{m_t}=\frac{m_t}{1-\beta_t}$

$V_t=\beta_2*V_{step-1}+(1-\beta_2)*g_t ^2$

ä¿®æ­£äºŒé˜¶åŠ¨é‡çš„åå·®ï¼š$\hat{V_t}=\frac {V_t}{1-\beta_2 ^t}$

$\eta_t = lr *\hat{m_t}/\sqrt{\hat{V_t}}=lr*\frac{m_t}{1-\beta_1 ^t} / \sqrt{\frac{V_t}{1-\beta_2 ^t}}$

$w_{t+1} = w_t - \eta_t = w_t-lr*\frac{m_t}{1-\beta_1^t}/ \sqrt{\frac{V_t}{1-\beta_2 ^t}}$

```python
# ä¸SGDä¸åŒä¹‹å¤„
m_w, m_b = 0, 0
v_w, v_b = 0, 0
beta1, beta2 = 0.9, 0.999
delta_w, delta_b = 0, 0
global_step = 0  # åé¢ +1
# adam
m_w = beta1 * m_w + (1 - beta1) * grads[0]
m_b = beta1 * m_b + (1 - beta1) * grads[1]
v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])
v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])

m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))
m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))
v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))
v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))

w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))
b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))
```

```
è¿è¡Œç»“æœï¼š
--------------------------
Epoch 498, loss: 0.013929554144851863
Test_acc: 1.0
--------------------------
Epoch 499, loss: 0.013926223502494395
Test_acc: 1.0
--------------------------
total_time 8.114784002304077
```





# ç¬¬ä¸‰ç«  ä½¿ç”¨å…«è‚¡æ­å»ºç¥ç»ç½‘ç»œ

* ç¥ç»ç½‘ç»œæ­å»ºå…«è‚¡
* irisä»£ç å¤ç°
* MNISTæ•°æ®é›†
* è®­ç»ƒMNISTæ•°æ®é›†
* Fashionæ•°æ®é›†



ç”¨ Tensorflow API: tf.keras æ­å»ºç½‘ç»œå…«è‚¡

å…­æ­¥æ³•

import 

train, test

model = tf.keras.models.Sequential

model.compile

model.fit

model.summray